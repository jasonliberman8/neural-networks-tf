{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![text](images/neuron.jpg \"Neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a neuron has n inputs and for each input has a weight. The first thing a neuron will do is compute the weighted average of the inputs based on its weights. Once it has done that, it will typically have an activation function that will somehow transform the weighted average that the neuron has computed. It will then output the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why is this called a neuron? Well, it somewhat corresponds to the structure of a neuron in the brain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](images/brain-neuron.png \"Brain Neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "    \n",
    "* Inputs: [10, 0, -5]\n",
    "* Weights: [1/3, 1/3, 1/3]\n",
    "* Activation: sign(x)\n",
    "\n",
    "What will be the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few functions that are commonly used as activation functions for neurons in neural networks.\n",
    "\n",
    "For example, the relu function and the sigmoid function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU | Sigmoid\n",
    "- | - \n",
    "![text](images/relu.png \"Relu\") | ![text](images/sigmoid.png \"Brain Neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting neurons to create a network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now that we understand what a single neuron is doing, a network is not that much more complicated. It is simply a number of layers of neurons connected to each other via weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](images/nn-example-nums.png \"nn-examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory you can have as many hidden layers as you want. And each neuron in layer L will have a weight for every single neuron in layer L - 1. This architecture is called a 'fully-connected' neuron network because every single neuron is connected via a weight to all of the neurons in the prior layer. In other architectures this might not be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer is also really important, and will look different depending on the specific problem you are trying to solve. Today, we will be doing classification. That is, we will have an image, and we will need to classify it as one of 10 types of articles of clothing. We will actually want our output layer to output the probability of each image belonging to each class, that is we want a 10-dimensional vector where the sum of the vector is 1 and the number in each row corresponds to the probability of the image belonging to that class.\n",
    "\n",
    "For other problems, you might want output layers that look different. For example, you might want a single neuron that outputs a number if you are trying to predict housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through an example of a neural network that can do multi-class classification. The input layer will be whatever inputs we have. In our case it's going to be all of the pixels of different images, but of course the inputs will vary for every problem. Maybe we have information about voters (Age, Address, Income) and want to predict Democrat, Republican, Independent, etc.\n",
    "\n",
    "1) Input layer: pixels of a photograph <br />\n",
    "2) Hidden layer 1: Each neuron is weighted sum of layer 1 with ReLU activation <br />\n",
    "3) Hidden layer 2: Each neuron is weighted sum of layer 2 with ReLU activation <br />\n",
    "4) Output layer: 10 neurons, each is a weighted sum of all the neurons in layer 2 with softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward process review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See whiteboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now that we have a way to calculate a prediction, how do we figure out the best weights? Well, we can actually model the problem with a cost function and then choose the weights that minimize the cost.\n",
    "\n",
    "Our cost function today will be the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](images/cost.png \"cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wouldn't worry too much about the specifics, if you are interested look up Softmax Regression, but intuitively, what is going on is the following: \n",
    "\n",
    "* The cost function is a sum of the cost of all of my training examples (hence sum from i=1 to m)\n",
    "* The cost of a single example is some measure of how wrong my prediction was. The more wrong I am, the more cost I incur.\n",
    "* The cost is a function of the weights to the prior layer and the outputs of the prior layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now we are ready to talk about the entire algorithm that we will implement today:\n",
    "    \n",
    "1) Randomly initiatilize all weights <br />\n",
    "2) Make predictions based on curent weights <br />\n",
    "3) Evaluate how I am doing using the cost function <br />\n",
    "4) Use calculus / optimization to find slightly better weights <br />\n",
    "5) Repeat from 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know that was a lot and we went very quickly, want to make sure everybody is good up to this point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a few minutes and make sure you have all the installs you need: https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Tensorflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a powerful open source library for numerical computation, particularly well suited and fine-tuned for large-scale Machine Learning.\n",
    "\n",
    "The basic principle of TensorFlow is simple - defined a computation graph for your calculation (for example, the one below), and TensorFlow takes that graph and runs it efficiently. It can also do autodifferentiation of your computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text](images/computation-graph.png \"cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Other background information about TensorFlow:\n",
    "\n",
    "* Created and maintained by Google\n",
    "* Open-sourced in 2015\n",
    "* Can run computation graphs across many CPUs / GPUs\n",
    "* API in python that is compatibile with other ML libraries like Scikit-Learn\n",
    "* Other high-level APIs have been built on top of TensorFlow (e.g. Keras)\n",
    "* Operations are implemented in C++\n",
    "* Can optimize arbitrary functions using automatic differentiating (or autodiff)\n",
    "* Particularly well-suited for Deep Neural Networks\n",
    "* Off the shelf visualization tool called TensorBoard\n",
    "* Popular open-source project on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.Variable(3, name=\"a\")             # Initializes a variable\n",
    "b = tf.Variable(4, name=\"b\")\n",
    "w = tf.constant(3)                       # Initializes a constant\n",
    "\n",
    "\n",
    "c = a+b                                  # Variabes can be functions of other variables and constants\n",
    "d = b-1\n",
    "e = c*d\n",
    "\n",
    "init = tf.global_variables_initializer() # adds step in computation graph to first initialize all the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above does not compute anything! It simply creates a computation graph. This is what is called the \"construction phase\" of a TensorFlow program.\n",
    "\n",
    "To run a computation, you need to open what is called a TensorFlow session and use it to execute the computation graph. The session will actually place the operations onto CPUs / GPUs and running them. This step is called the \"execution phase\" of a TensorFlow program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:   # This is where stuff actually happens\n",
    "    init.run()               # actually initializes the variables\n",
    "    result = e.eval()        # evaluates the computation graph\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow, you can also create a placeholder, which is an object whose value you will specify later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the value of x in the feed_dict\n",
    "with tf.Session() as sess:\n",
    "    x = tf.placeholder(tf.int64, name = 'x')\n",
    "    print(sess.run(2 * x, feed_dict = {x: 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I have the following matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)         # To ensure that we all get the same answers\n",
    "    \n",
    "X = np.random.randn(3, 1)  # X is a 3 x 1 matrix\n",
    "W = np.random.randn(4, 3)  # W is a 4 x 3 matrix\n",
    "b = np.random.randn(4, 1)  # b is a 4 x 1 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Y = W * X + b\n",
    "\n",
    "You might want to use:\n",
    "- tf.matmul(..., ...) to do a matrix multiplication\n",
    "- tf.add(..., ...) to do an addition (though '+' should also work just fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # Implement Y\n",
    "    Y = tf.matmul(W, X) + b\n",
    "    \n",
    "    init.run()                 \n",
    "    result = Y.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Expected Output ***: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "**result**\n",
    "</td>\n",
    "<td>\n",
    "[[-2.15657382]\n",
    " [ 2.95891446]\n",
    " [-1.08926781]\n",
    " [-0.84538042]]\n",
    "</td>\n",
    "</tr> \n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OK - Let's Build a Neural Network from Scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the things we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.mnist_reader import load_mnist\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's pull in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_raw, y_train_raw = load_mnist('data/fashion', kind='train')\n",
    "X_test_raw, y_test_raw = load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test_raw[i].reshape(28,28), cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data between 0 and 1 - this speeds up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_raw.astype('float32') / 255\n",
    "X_test = X_test_raw.astype('float32') / 255\n",
    "y_train = y_train_raw.astype('float32')\n",
    "y_test = y_test_raw.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # Number of inputs for each training example\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 30\n",
    "n_outputs = 10    # Notice this matches the number of classes we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to write a function that will take in the results of a prior layer in our NN and output the results of the next layer. The function will take in the results of the prior layer - X, the number of neurons we want for that layer, and the activation function, and returns the output of the layer.\n",
    "\n",
    "Implementation details:\n",
    "\n",
    "* b should be a tensorflow variable of a n_neurons dimensional zero vector (you might want to use tf.zeros, and make sure you pass it the argument name=\"b\")\n",
    "* Z = X * W + b\n",
    "* W has dimensions (Number of inputs from prior layer) x (Number of neurons in this layer).\n",
    "* If activation is None, use the identity function, otherwise use the function passed in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    \"\"\"\n",
    "    Creates a generic layer of neurons.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- the results of the prior layer. X is M x N where M = instances and N = number of neurons in prior layer\n",
    "    n_neurons -- the number of neurons desired for the layer.\n",
    "    activation -- the activation function in the layer. If None, activation is the identity function.\n",
    "    \n",
    "    Returns:\n",
    "    Z = activation(X * W + b)\n",
    "    \"\"\"\n",
    "    #Create name for layer - helpful for viz later\n",
    "    with tf.variable_scope(name):    \n",
    "        \n",
    "        # Gets the number of inputs for each instance\n",
    "        n_inputs = int(X.get_shape()[1])      \n",
    "        \n",
    "        # The following three lines create a matrix of weights, such that for \n",
    "        # each input i from the prior layer and each neuron j in this layer, W[i, j] will be the weight\n",
    "        # of the input i for neuron j.\n",
    "        #\n",
    "        # For technical reasons, we want to initialize our weights randomly, and not only that, but\n",
    "        # in practice, specific distributions have been shown to improve training significantly. One\n",
    "        # of the used initialization schemes is called Xavier initialization, which we use here.\n",
    "        W = tf.get_variable(\"W\", [n_inputs, n_neurons], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        \n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Construct the NN architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so now all we have to do is construct our actual layers. For each layer, call the neuron_layer function. For the hidden layers use tf.nn.relu as the activation. But for the output layer,  Make sure to pass in names for all of your layers. For each layer, call the neuron_layer function. However, for the final layer we will use no activation function for now. The reason for this is that tensorflow will handle the softmax portion of this under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"DNN\"): #DNN = Deep Neural Network!\n",
    "    \n",
    "    # Our first hidden layer\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation = tf.nn.relu)\n",
    "    \n",
    "    # Second hidden layer\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation = tf.nn.relu)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    # Defining our loss function\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "    loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    \n",
    "    # Define an optimizer for our loss function - here we use GradientDescent, but we could use many others!\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    # What do we want our optimizer to do?\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion - what is backpropogation? How does optimization work on neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(outputs, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function we discussed above was expressed as a sum of all of the training examples. However, in practice it is often a lot faster to use small chunks of the overall training example at once. So our process would look like this:\n",
    "\n",
    "1) Randomly initiatilize all weights <br />\n",
    "2) Make predictions based on curent weights for some small batch of random training examples <br />\n",
    "3) Evaluate how I am doing using the cost function on this batch of random training examples <br />\n",
    "4) Use calculus / optimization to find slightly better weights <br />\n",
    "5) Repeat from 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of num random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , data.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = how many times will I look at all of my examples\n",
    "n_epochs = 30\n",
    "# batch_size = how many examples will be in each batch\n",
    "batch_size = 32\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(X_train.shape[0] // batch_size):\n",
    "            X_batch, y_batch = next_batch(batch_size, X_train, y_train)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            #writer.add_summary(summary, epoch * batch_size + iteration)\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our neural network is overfitting! Let's apply dropout!\n",
    "\n",
    "Dropout is a way of preventing overfitting of a neural network and is frequently used in practice. The concept is fairly simple: at every training step, every neuron in the network (excluding the output neurons) has a probability p of being dropped out - meaning that it will be entirely ignored in this training step. The hyperparameter p is called the dropout rate and is typically set in the 25-50% range.\n",
    "\n",
    "Why does dropout work?\n",
    "\n",
    "* Prevents neurons from co-adapting with neighboring neurons and learning complex, fluke-y features from random noise\n",
    "* Neurons are less sensitive to small changes in inputs\n",
    "* Neurons cannot rely on any single feature to make predictions, but rather must learn different and independent features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.25\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"DNN\"): #DNN = Deep Neural Network!\n",
    "    \n",
    "    # Our first hidden layer\n",
    "    hidden1 = neuron_layer(X_drop, n_hidden1, name=\"hidden1\", activation = tf.nn.relu)\n",
    "    \n",
    "    hidden1_drop = tf.layers.dense(hidden1, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    \n",
    "    # Second hidden layer\n",
    "    hidden2 = neuron_layer(hidden1_drop, n_hidden2, name=\"hidden2\", activation = tf.nn.relu)\n",
    "    \n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = neuron_layer(hidden2_drop, n_outputs, name=\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.03\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    \n",
    "    # Define an optimizer - here we use GradientDescent, but we could use many others!\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    # What do we want our optimizer to do?\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up viz\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(X_train.shape[0] // batch_size):\n",
    "            X_batch, y_batch = next_batch(batch_size, X_train, y_train)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training:True})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
